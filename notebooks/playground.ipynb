{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from windmark.core.structs import LevelSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "levelset = LevelSet(\"event_type\", [\"hello\", \"world\", \"oh_yeah\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<enum 'LevelEnum'>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "levelset.mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (9, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>statistic</th><th>ham</th></tr><tr><td>str</td><td>f64</td></tr></thead><tbody><tr><td>&quot;count&quot;</td><td>100000.0</td></tr><tr><td>&quot;null_count&quot;</td><td>0.0</td></tr><tr><td>&quot;mean&quot;</td><td>0.498969</td></tr><tr><td>&quot;std&quot;</td><td>0.288691</td></tr><tr><td>&quot;min&quot;</td><td>0.000014</td></tr><tr><td>&quot;25%&quot;</td><td>0.248285</td></tr><tr><td>&quot;50%&quot;</td><td>0.499331</td></tr><tr><td>&quot;75%&quot;</td><td>0.748348</td></tr><tr><td>&quot;max&quot;</td><td>0.999999</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (9, 2)\n",
       "┌────────────┬──────────┐\n",
       "│ statistic  ┆ ham      │\n",
       "│ ---        ┆ ---      │\n",
       "│ str        ┆ f64      │\n",
       "╞════════════╪══════════╡\n",
       "│ count      ┆ 100000.0 │\n",
       "│ null_count ┆ 0.0      │\n",
       "│ mean       ┆ 0.498969 │\n",
       "│ std        ┆ 0.288691 │\n",
       "│ min        ┆ 0.000014 │\n",
       "│ 25%        ┆ 0.248285 │\n",
       "│ 50%        ┆ 0.499331 │\n",
       "│ 75%        ┆ 0.748348 │\n",
       "│ max        ┆ 0.999999 │\n",
       "└────────────┴──────────┘"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from zlib import crc32\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "import random\n",
    "import string\n",
    "\n",
    "\n",
    "def generate_random_string_list(size, str_length):\n",
    "    \"\"\"Generates a list of random strings.\n",
    "\n",
    "    Args:\n",
    "        size (int): The number of random strings to generate.\n",
    "        str_length (int): The length of each string.\n",
    "\n",
    "    Returns:\n",
    "        list: A list containing 'size' random strings, each 'str_length' characters long.\n",
    "    \"\"\"\n",
    "    # Create a list of 'size' random strings, each 'str_length' characters long\n",
    "    random_strings = [\"\".join(random.choices(string.ascii_letters + string.digits, k=str_length)) for _ in range(size)]\n",
    "    return random_strings\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "strings = generate_random_string_list(100000, 10)  # Generates 10,000 random strings, each 10 characters long.\n",
    "\n",
    "df = pl.DataFrame({\"ham\": strings})\n",
    "\n",
    "df.select(\n",
    "    pl.col(\"ham\")\n",
    "    .map_elements(lambda x: float(crc32(str.encode(x)) & 0xFFFFFFFF), return_dtype=pl.Float32)\n",
    "    .mul(1 / 2**32)\n",
    ").describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModuleDict(\n",
      "  (train_collection): MetricCollection(\n",
      "    (acc): MulticlassAccuracy()\n",
      "    (ap): MulticlassAveragePrecision()\n",
      "    (auc): MulticlassAUROC()\n",
      "    (f1): MulticlassF1Score()\n",
      "  )\n",
      "  (validate_collection): MetricCollection(\n",
      "    (acc): MulticlassAccuracy()\n",
      "    (ap): MulticlassAveragePrecision()\n",
      "    (auc): MulticlassAUROC()\n",
      "    (f1): MulticlassF1Score()\n",
      "  )\n",
      "  (test_collection): MetricCollection(\n",
      "    (acc): MulticlassAccuracy()\n",
      "    (ap): MulticlassAveragePrecision()\n",
      "    (auc): MulticlassAUROC()\n",
      "    (f1): MulticlassF1Score()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchmetrics\n",
    "\n",
    "\n",
    "metrics = dict(\n",
    "    ap=torchmetrics.AveragePrecision,\n",
    "    f1=torchmetrics.F1Score,\n",
    "    auc=torchmetrics.AUROC,\n",
    "    acc=torchmetrics.Accuracy,\n",
    ")\n",
    "\n",
    "collection = torchmetrics.MetricCollection(\n",
    "    {name: metric(task=\"multiclass\", num_classes=2) for name, metric in metrics.items()}\n",
    ")\n",
    "\n",
    "collections = torch.nn.ModuleDict({\n",
    "    \"train_collection\": collection.clone(),\n",
    "    \"validate_collection\": collection.clone(),\n",
    "    \"test_collection\": collection.clone(),\n",
    "})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc\n",
      "MulticlassAccuracy()\n",
      "ap\n",
      "MulticlassAveragePrecision()\n",
      "auc\n",
      "MulticlassAUROC()\n",
      "f1\n",
      "MulticlassF1Score()\n"
     ]
    }
   ],
   "source": [
    "for name, metric in collection.items():\n",
    "    print(name)\n",
    "    print(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "windmark-gThMBSDt-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
