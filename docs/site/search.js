window.pdocSearch = (function(){
/** elasticlunr - http://weixsong.github.io * Copyright (C) 2017 Oliver Nightingale * Copyright (C) 2017 Wei Song * MIT Licensed */!function(){function e(e){if(null===e||"object"!=typeof e)return e;var t=e.constructor();for(var n in e)e.hasOwnProperty(n)&&(t[n]=e[n]);return t}var t=function(e){var n=new t.Index;return n.pipeline.add(t.trimmer,t.stopWordFilter,t.stemmer),e&&e.call(n,n),n};t.version="0.9.5",lunr=t,t.utils={},t.utils.warn=function(e){return function(t){e.console&&console.warn&&console.warn(t)}}(this),t.utils.toString=function(e){return void 0===e||null===e?"":e.toString()},t.EventEmitter=function(){this.events={}},t.EventEmitter.prototype.addListener=function(){var e=Array.prototype.slice.call(arguments),t=e.pop(),n=e;if("function"!=typeof t)throw new TypeError("last argument must be a function");n.forEach(function(e){this.hasHandler(e)||(this.events[e]=[]),this.events[e].push(t)},this)},t.EventEmitter.prototype.removeListener=function(e,t){if(this.hasHandler(e)){var n=this.events[e].indexOf(t);-1!==n&&(this.events[e].splice(n,1),0==this.events[e].length&&delete this.events[e])}},t.EventEmitter.prototype.emit=function(e){if(this.hasHandler(e)){var t=Array.prototype.slice.call(arguments,1);this.events[e].forEach(function(e){e.apply(void 0,t)},this)}},t.EventEmitter.prototype.hasHandler=function(e){return e in this.events},t.tokenizer=function(e){if(!arguments.length||null===e||void 0===e)return[];if(Array.isArray(e)){var n=e.filter(function(e){return null===e||void 0===e?!1:!0});n=n.map(function(e){return t.utils.toString(e).toLowerCase()});var i=[];return n.forEach(function(e){var n=e.split(t.tokenizer.seperator);i=i.concat(n)},this),i}return e.toString().trim().toLowerCase().split(t.tokenizer.seperator)},t.tokenizer.defaultSeperator=/[\s\-]+/,t.tokenizer.seperator=t.tokenizer.defaultSeperator,t.tokenizer.setSeperator=function(e){null!==e&&void 0!==e&&"object"==typeof e&&(t.tokenizer.seperator=e)},t.tokenizer.resetSeperator=function(){t.tokenizer.seperator=t.tokenizer.defaultSeperator},t.tokenizer.getSeperator=function(){return t.tokenizer.seperator},t.Pipeline=function(){this._queue=[]},t.Pipeline.registeredFunctions={},t.Pipeline.registerFunction=function(e,n){n in t.Pipeline.registeredFunctions&&t.utils.warn("Overwriting existing registered function: "+n),e.label=n,t.Pipeline.registeredFunctions[n]=e},t.Pipeline.getRegisteredFunction=function(e){return e in t.Pipeline.registeredFunctions!=!0?null:t.Pipeline.registeredFunctions[e]},t.Pipeline.warnIfFunctionNotRegistered=function(e){var n=e.label&&e.label in this.registeredFunctions;n||t.utils.warn("Function is not registered with pipeline. This may cause problems when serialising the index.\n",e)},t.Pipeline.load=function(e){var n=new t.Pipeline;return e.forEach(function(e){var i=t.Pipeline.getRegisteredFunction(e);if(!i)throw new Error("Cannot load un-registered function: "+e);n.add(i)}),n},t.Pipeline.prototype.add=function(){var e=Array.prototype.slice.call(arguments);e.forEach(function(e){t.Pipeline.warnIfFunctionNotRegistered(e),this._queue.push(e)},this)},t.Pipeline.prototype.after=function(e,n){t.Pipeline.warnIfFunctionNotRegistered(n);var i=this._queue.indexOf(e);if(-1===i)throw new Error("Cannot find existingFn");this._queue.splice(i+1,0,n)},t.Pipeline.prototype.before=function(e,n){t.Pipeline.warnIfFunctionNotRegistered(n);var i=this._queue.indexOf(e);if(-1===i)throw new Error("Cannot find existingFn");this._queue.splice(i,0,n)},t.Pipeline.prototype.remove=function(e){var t=this._queue.indexOf(e);-1!==t&&this._queue.splice(t,1)},t.Pipeline.prototype.run=function(e){for(var t=[],n=e.length,i=this._queue.length,o=0;n>o;o++){for(var r=e[o],s=0;i>s&&(r=this._queue[s](r,o,e),void 0!==r&&null!==r);s++);void 0!==r&&null!==r&&t.push(r)}return t},t.Pipeline.prototype.reset=function(){this._queue=[]},t.Pipeline.prototype.get=function(){return this._queue},t.Pipeline.prototype.toJSON=function(){return this._queue.map(function(e){return t.Pipeline.warnIfFunctionNotRegistered(e),e.label})},t.Index=function(){this._fields=[],this._ref="id",this.pipeline=new t.Pipeline,this.documentStore=new t.DocumentStore,this.index={},this.eventEmitter=new t.EventEmitter,this._idfCache={},this.on("add","remove","update",function(){this._idfCache={}}.bind(this))},t.Index.prototype.on=function(){var e=Array.prototype.slice.call(arguments);return this.eventEmitter.addListener.apply(this.eventEmitter,e)},t.Index.prototype.off=function(e,t){return this.eventEmitter.removeListener(e,t)},t.Index.load=function(e){e.version!==t.version&&t.utils.warn("version mismatch: current "+t.version+" importing "+e.version);var n=new this;n._fields=e.fields,n._ref=e.ref,n.documentStore=t.DocumentStore.load(e.documentStore),n.pipeline=t.Pipeline.load(e.pipeline),n.index={};for(var i in e.index)n.index[i]=t.InvertedIndex.load(e.index[i]);return n},t.Index.prototype.addField=function(e){return this._fields.push(e),this.index[e]=new t.InvertedIndex,this},t.Index.prototype.setRef=function(e){return this._ref=e,this},t.Index.prototype.saveDocument=function(e){return this.documentStore=new t.DocumentStore(e),this},t.Index.prototype.addDoc=function(e,n){if(e){var n=void 0===n?!0:n,i=e[this._ref];this.documentStore.addDoc(i,e),this._fields.forEach(function(n){var o=this.pipeline.run(t.tokenizer(e[n]));this.documentStore.addFieldLength(i,n,o.length);var r={};o.forEach(function(e){e in r?r[e]+=1:r[e]=1},this);for(var s in r){var u=r[s];u=Math.sqrt(u),this.index[n].addToken(s,{ref:i,tf:u})}},this),n&&this.eventEmitter.emit("add",e,this)}},t.Index.prototype.removeDocByRef=function(e){if(e&&this.documentStore.isDocStored()!==!1&&this.documentStore.hasDoc(e)){var t=this.documentStore.getDoc(e);this.removeDoc(t,!1)}},t.Index.prototype.removeDoc=function(e,n){if(e){var n=void 0===n?!0:n,i=e[this._ref];this.documentStore.hasDoc(i)&&(this.documentStore.removeDoc(i),this._fields.forEach(function(n){var o=this.pipeline.run(t.tokenizer(e[n]));o.forEach(function(e){this.index[n].removeToken(e,i)},this)},this),n&&this.eventEmitter.emit("remove",e,this))}},t.Index.prototype.updateDoc=function(e,t){var t=void 0===t?!0:t;this.removeDocByRef(e[this._ref],!1),this.addDoc(e,!1),t&&this.eventEmitter.emit("update",e,this)},t.Index.prototype.idf=function(e,t){var n="@"+t+"/"+e;if(Object.prototype.hasOwnProperty.call(this._idfCache,n))return this._idfCache[n];var i=this.index[t].getDocFreq(e),o=1+Math.log(this.documentStore.length/(i+1));return this._idfCache[n]=o,o},t.Index.prototype.getFields=function(){return this._fields.slice()},t.Index.prototype.search=function(e,n){if(!e)return[];e="string"==typeof e?{any:e}:JSON.parse(JSON.stringify(e));var i=null;null!=n&&(i=JSON.stringify(n));for(var o=new t.Configuration(i,this.getFields()).get(),r={},s=Object.keys(e),u=0;u<s.length;u++){var a=s[u];r[a]=this.pipeline.run(t.tokenizer(e[a]))}var l={};for(var c in o){var d=r[c]||r.any;if(d){var f=this.fieldSearch(d,c,o),h=o[c].boost;for(var p in f)f[p]=f[p]*h;for(var p in f)p in l?l[p]+=f[p]:l[p]=f[p]}}var v,g=[];for(var p in l)v={ref:p,score:l[p]},this.documentStore.hasDoc(p)&&(v.doc=this.documentStore.getDoc(p)),g.push(v);return g.sort(function(e,t){return t.score-e.score}),g},t.Index.prototype.fieldSearch=function(e,t,n){var i=n[t].bool,o=n[t].expand,r=n[t].boost,s=null,u={};return 0!==r?(e.forEach(function(e){var n=[e];1==o&&(n=this.index[t].expandToken(e));var r={};n.forEach(function(n){var o=this.index[t].getDocs(n),a=this.idf(n,t);if(s&&"AND"==i){var l={};for(var c in s)c in o&&(l[c]=o[c]);o=l}n==e&&this.fieldSearchStats(u,n,o);for(var c in o){var d=this.index[t].getTermFrequency(n,c),f=this.documentStore.getFieldLength(c,t),h=1;0!=f&&(h=1/Math.sqrt(f));var p=1;n!=e&&(p=.15*(1-(n.length-e.length)/n.length));var v=d*a*h*p;c in r?r[c]+=v:r[c]=v}},this),s=this.mergeScores(s,r,i)},this),s=this.coordNorm(s,u,e.length)):void 0},t.Index.prototype.mergeScores=function(e,t,n){if(!e)return t;if("AND"==n){var i={};for(var o in t)o in e&&(i[o]=e[o]+t[o]);return i}for(var o in t)o in e?e[o]+=t[o]:e[o]=t[o];return e},t.Index.prototype.fieldSearchStats=function(e,t,n){for(var i in n)i in e?e[i].push(t):e[i]=[t]},t.Index.prototype.coordNorm=function(e,t,n){for(var i in e)if(i in t){var o=t[i].length;e[i]=e[i]*o/n}return e},t.Index.prototype.toJSON=function(){var e={};return this._fields.forEach(function(t){e[t]=this.index[t].toJSON()},this),{version:t.version,fields:this._fields,ref:this._ref,documentStore:this.documentStore.toJSON(),index:e,pipeline:this.pipeline.toJSON()}},t.Index.prototype.use=function(e){var t=Array.prototype.slice.call(arguments,1);t.unshift(this),e.apply(this,t)},t.DocumentStore=function(e){this._save=null===e||void 0===e?!0:e,this.docs={},this.docInfo={},this.length=0},t.DocumentStore.load=function(e){var t=new this;return t.length=e.length,t.docs=e.docs,t.docInfo=e.docInfo,t._save=e.save,t},t.DocumentStore.prototype.isDocStored=function(){return this._save},t.DocumentStore.prototype.addDoc=function(t,n){this.hasDoc(t)||this.length++,this.docs[t]=this._save===!0?e(n):null},t.DocumentStore.prototype.getDoc=function(e){return this.hasDoc(e)===!1?null:this.docs[e]},t.DocumentStore.prototype.hasDoc=function(e){return e in this.docs},t.DocumentStore.prototype.removeDoc=function(e){this.hasDoc(e)&&(delete this.docs[e],delete this.docInfo[e],this.length--)},t.DocumentStore.prototype.addFieldLength=function(e,t,n){null!==e&&void 0!==e&&0!=this.hasDoc(e)&&(this.docInfo[e]||(this.docInfo[e]={}),this.docInfo[e][t]=n)},t.DocumentStore.prototype.updateFieldLength=function(e,t,n){null!==e&&void 0!==e&&0!=this.hasDoc(e)&&this.addFieldLength(e,t,n)},t.DocumentStore.prototype.getFieldLength=function(e,t){return null===e||void 0===e?0:e in this.docs&&t in this.docInfo[e]?this.docInfo[e][t]:0},t.DocumentStore.prototype.toJSON=function(){return{docs:this.docs,docInfo:this.docInfo,length:this.length,save:this._save}},t.stemmer=function(){var e={ational:"ate",tional:"tion",enci:"ence",anci:"ance",izer:"ize",bli:"ble",alli:"al",entli:"ent",eli:"e",ousli:"ous",ization:"ize",ation:"ate",ator:"ate",alism:"al",iveness:"ive",fulness:"ful",ousness:"ous",aliti:"al",iviti:"ive",biliti:"ble",logi:"log"},t={icate:"ic",ative:"",alize:"al",iciti:"ic",ical:"ic",ful:"",ness:""},n="[^aeiou]",i="[aeiouy]",o=n+"[^aeiouy]*",r=i+"[aeiou]*",s="^("+o+")?"+r+o,u="^("+o+")?"+r+o+"("+r+")?$",a="^("+o+")?"+r+o+r+o,l="^("+o+")?"+i,c=new RegExp(s),d=new RegExp(a),f=new RegExp(u),h=new RegExp(l),p=/^(.+?)(ss|i)es$/,v=/^(.+?)([^s])s$/,g=/^(.+?)eed$/,m=/^(.+?)(ed|ing)$/,y=/.$/,S=/(at|bl|iz)$/,x=new RegExp("([^aeiouylsz])\\1$"),w=new RegExp("^"+o+i+"[^aeiouwxy]$"),I=/^(.+?[^aeiou])y$/,b=/^(.+?)(ational|tional|enci|anci|izer|bli|alli|entli|eli|ousli|ization|ation|ator|alism|iveness|fulness|ousness|aliti|iviti|biliti|logi)$/,E=/^(.+?)(icate|ative|alize|iciti|ical|ful|ness)$/,D=/^(.+?)(al|ance|ence|er|ic|able|ible|ant|ement|ment|ent|ou|ism|ate|iti|ous|ive|ize)$/,F=/^(.+?)(s|t)(ion)$/,_=/^(.+?)e$/,P=/ll$/,k=new RegExp("^"+o+i+"[^aeiouwxy]$"),z=function(n){var i,o,r,s,u,a,l;if(n.length<3)return n;if(r=n.substr(0,1),"y"==r&&(n=r.toUpperCase()+n.substr(1)),s=p,u=v,s.test(n)?n=n.replace(s,"$1$2"):u.test(n)&&(n=n.replace(u,"$1$2")),s=g,u=m,s.test(n)){var z=s.exec(n);s=c,s.test(z[1])&&(s=y,n=n.replace(s,""))}else if(u.test(n)){var z=u.exec(n);i=z[1],u=h,u.test(i)&&(n=i,u=S,a=x,l=w,u.test(n)?n+="e":a.test(n)?(s=y,n=n.replace(s,"")):l.test(n)&&(n+="e"))}if(s=I,s.test(n)){var z=s.exec(n);i=z[1],n=i+"i"}if(s=b,s.test(n)){var z=s.exec(n);i=z[1],o=z[2],s=c,s.test(i)&&(n=i+e[o])}if(s=E,s.test(n)){var z=s.exec(n);i=z[1],o=z[2],s=c,s.test(i)&&(n=i+t[o])}if(s=D,u=F,s.test(n)){var z=s.exec(n);i=z[1],s=d,s.test(i)&&(n=i)}else if(u.test(n)){var z=u.exec(n);i=z[1]+z[2],u=d,u.test(i)&&(n=i)}if(s=_,s.test(n)){var z=s.exec(n);i=z[1],s=d,u=f,a=k,(s.test(i)||u.test(i)&&!a.test(i))&&(n=i)}return s=P,u=d,s.test(n)&&u.test(n)&&(s=y,n=n.replace(s,"")),"y"==r&&(n=r.toLowerCase()+n.substr(1)),n};return z}(),t.Pipeline.registerFunction(t.stemmer,"stemmer"),t.stopWordFilter=function(e){return e&&t.stopWordFilter.stopWords[e]!==!0?e:void 0},t.clearStopWords=function(){t.stopWordFilter.stopWords={}},t.addStopWords=function(e){null!=e&&Array.isArray(e)!==!1&&e.forEach(function(e){t.stopWordFilter.stopWords[e]=!0},this)},t.resetStopWords=function(){t.stopWordFilter.stopWords=t.defaultStopWords},t.defaultStopWords={"":!0,a:!0,able:!0,about:!0,across:!0,after:!0,all:!0,almost:!0,also:!0,am:!0,among:!0,an:!0,and:!0,any:!0,are:!0,as:!0,at:!0,be:!0,because:!0,been:!0,but:!0,by:!0,can:!0,cannot:!0,could:!0,dear:!0,did:!0,"do":!0,does:!0,either:!0,"else":!0,ever:!0,every:!0,"for":!0,from:!0,get:!0,got:!0,had:!0,has:!0,have:!0,he:!0,her:!0,hers:!0,him:!0,his:!0,how:!0,however:!0,i:!0,"if":!0,"in":!0,into:!0,is:!0,it:!0,its:!0,just:!0,least:!0,let:!0,like:!0,likely:!0,may:!0,me:!0,might:!0,most:!0,must:!0,my:!0,neither:!0,no:!0,nor:!0,not:!0,of:!0,off:!0,often:!0,on:!0,only:!0,or:!0,other:!0,our:!0,own:!0,rather:!0,said:!0,say:!0,says:!0,she:!0,should:!0,since:!0,so:!0,some:!0,than:!0,that:!0,the:!0,their:!0,them:!0,then:!0,there:!0,these:!0,they:!0,"this":!0,tis:!0,to:!0,too:!0,twas:!0,us:!0,wants:!0,was:!0,we:!0,were:!0,what:!0,when:!0,where:!0,which:!0,"while":!0,who:!0,whom:!0,why:!0,will:!0,"with":!0,would:!0,yet:!0,you:!0,your:!0},t.stopWordFilter.stopWords=t.defaultStopWords,t.Pipeline.registerFunction(t.stopWordFilter,"stopWordFilter"),t.trimmer=function(e){if(null===e||void 0===e)throw new Error("token should not be undefined");return e.replace(/^\W+/,"").replace(/\W+$/,"")},t.Pipeline.registerFunction(t.trimmer,"trimmer"),t.InvertedIndex=function(){this.root={docs:{},df:0}},t.InvertedIndex.load=function(e){var t=new this;return t.root=e.root,t},t.InvertedIndex.prototype.addToken=function(e,t,n){for(var n=n||this.root,i=0;i<=e.length-1;){var o=e[i];o in n||(n[o]={docs:{},df:0}),i+=1,n=n[o]}var r=t.ref;n.docs[r]?n.docs[r]={tf:t.tf}:(n.docs[r]={tf:t.tf},n.df+=1)},t.InvertedIndex.prototype.hasToken=function(e){if(!e)return!1;for(var t=this.root,n=0;n<e.length;n++){if(!t[e[n]])return!1;t=t[e[n]]}return!0},t.InvertedIndex.prototype.getNode=function(e){if(!e)return null;for(var t=this.root,n=0;n<e.length;n++){if(!t[e[n]])return null;t=t[e[n]]}return t},t.InvertedIndex.prototype.getDocs=function(e){var t=this.getNode(e);return null==t?{}:t.docs},t.InvertedIndex.prototype.getTermFrequency=function(e,t){var n=this.getNode(e);return null==n?0:t in n.docs?n.docs[t].tf:0},t.InvertedIndex.prototype.getDocFreq=function(e){var t=this.getNode(e);return null==t?0:t.df},t.InvertedIndex.prototype.removeToken=function(e,t){if(e){var n=this.getNode(e);null!=n&&t in n.docs&&(delete n.docs[t],n.df-=1)}},t.InvertedIndex.prototype.expandToken=function(e,t,n){if(null==e||""==e)return[];var t=t||[];if(void 0==n&&(n=this.getNode(e),null==n))return t;n.df>0&&t.push(e);for(var i in n)"docs"!==i&&"df"!==i&&this.expandToken(e+i,t,n[i]);return t},t.InvertedIndex.prototype.toJSON=function(){return{root:this.root}},t.Configuration=function(e,n){var e=e||"";if(void 0==n||null==n)throw new Error("fields should not be null");this.config={};var i;try{i=JSON.parse(e),this.buildUserConfig(i,n)}catch(o){t.utils.warn("user configuration parse failed, will use default configuration"),this.buildDefaultConfig(n)}},t.Configuration.prototype.buildDefaultConfig=function(e){this.reset(),e.forEach(function(e){this.config[e]={boost:1,bool:"OR",expand:!1}},this)},t.Configuration.prototype.buildUserConfig=function(e,n){var i="OR",o=!1;if(this.reset(),"bool"in e&&(i=e.bool||i),"expand"in e&&(o=e.expand||o),"fields"in e)for(var r in e.fields)if(n.indexOf(r)>-1){var s=e.fields[r],u=o;void 0!=s.expand&&(u=s.expand),this.config[r]={boost:s.boost||0===s.boost?s.boost:1,bool:s.bool||i,expand:u}}else t.utils.warn("field name in user configuration not found in index instance fields");else this.addAllFields2UserConfig(i,o,n)},t.Configuration.prototype.addAllFields2UserConfig=function(e,t,n){n.forEach(function(n){this.config[n]={boost:1,bool:e,expand:t}},this)},t.Configuration.prototype.get=function(){return this.config},t.Configuration.prototype.reset=function(){this.config={}},lunr.SortedSet=function(){this.length=0,this.elements=[]},lunr.SortedSet.load=function(e){var t=new this;return t.elements=e,t.length=e.length,t},lunr.SortedSet.prototype.add=function(){var e,t;for(e=0;e<arguments.length;e++)t=arguments[e],~this.indexOf(t)||this.elements.splice(this.locationFor(t),0,t);this.length=this.elements.length},lunr.SortedSet.prototype.toArray=function(){return this.elements.slice()},lunr.SortedSet.prototype.map=function(e,t){return this.elements.map(e,t)},lunr.SortedSet.prototype.forEach=function(e,t){return this.elements.forEach(e,t)},lunr.SortedSet.prototype.indexOf=function(e){for(var t=0,n=this.elements.length,i=n-t,o=t+Math.floor(i/2),r=this.elements[o];i>1;){if(r===e)return o;e>r&&(t=o),r>e&&(n=o),i=n-t,o=t+Math.floor(i/2),r=this.elements[o]}return r===e?o:-1},lunr.SortedSet.prototype.locationFor=function(e){for(var t=0,n=this.elements.length,i=n-t,o=t+Math.floor(i/2),r=this.elements[o];i>1;)e>r&&(t=o),r>e&&(n=o),i=n-t,o=t+Math.floor(i/2),r=this.elements[o];return r>e?o:e>r?o+1:void 0},lunr.SortedSet.prototype.intersect=function(e){for(var t=new lunr.SortedSet,n=0,i=0,o=this.length,r=e.length,s=this.elements,u=e.elements;;){if(n>o-1||i>r-1)break;s[n]!==u[i]?s[n]<u[i]?n++:s[n]>u[i]&&i++:(t.add(s[n]),n++,i++)}return t},lunr.SortedSet.prototype.clone=function(){var e=new lunr.SortedSet;return e.elements=this.toArray(),e.length=e.elements.length,e},lunr.SortedSet.prototype.union=function(e){var t,n,i;this.length>=e.length?(t=this,n=e):(t=e,n=this),i=t.clone();for(var o=0,r=n.toArray();o<r.length;o++)i.add(r[o]);return i},lunr.SortedSet.prototype.toJSON=function(){return this.toArray()},function(e,t){"function"==typeof define&&define.amd?define(t):"object"==typeof exports?module.exports=t():e.elasticlunr=t()}(this,function(){return t})}();
    /** pdoc search index */const docs = [{"fullname": "source.core.architecture", "modulename": "source.core.architecture", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "source.core.architecture.complexity", "modulename": "source.core.architecture", "qualname": "complexity", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">params</span><span class=\"p\">:</span> <span class=\"n\">source</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">schema</span><span class=\"o\">.</span><span class=\"n\">Hyperparameters</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">int</span>:</span></span>", "funcdef": "def"}, {"fullname": "source.core.architecture.create_attention_masks", "modulename": "source.core.architecture", "qualname": "create_attention_masks", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">inputs</span><span class=\"p\">:</span> <span class=\"n\">tensordict</span><span class=\"o\">.</span><span class=\"n\">_td</span><span class=\"o\">.</span><span class=\"n\">TensorDict</span>,</span><span class=\"param\">\t<span class=\"n\">fields</span><span class=\"p\">:</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"n\">source</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">schema</span><span class=\"o\">.</span><span class=\"n\">Field</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "source.core.architecture.LearnedTensor", "modulename": "source.core.architecture", "qualname": "LearnedTensor", "kind": "class", "doc": "<p>LearnedTensor is a PyTorch module that initializes a tensor with normal distribution and learns its values during training.</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "source.core.architecture.LearnedTensor.__init__", "modulename": "source.core.architecture", "qualname": "LearnedTensor.__init__", "kind": "function", "doc": "<p>Initializes the LearnedTensor.</p>\n\n<p>Args:\n    sizes (int): The sizes of the dimensions of the tensor.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"o\">*</span><span class=\"n\">sizes</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span>)</span>"}, {"fullname": "source.core.architecture.LearnedTensor.tensor", "modulename": "source.core.architecture", "qualname": "LearnedTensor.tensor", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "source.core.architecture.LearnedTensor.forward", "modulename": "source.core.architecture", "qualname": "LearnedTensor.forward", "kind": "function", "doc": "<p>Returns the learned tensor.</p>\n\n<p>Returns:\n    Float[Tensor, '...']: The learned tensor.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">jaxtyping</span><span class=\"o\">.</span><span class=\"n\">Float</span><span class=\"p\">[</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"s1\">&#39;...&#39;</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "source.core.architecture.DiscreteFieldEmbedder", "modulename": "source.core.architecture", "qualname": "DiscreteFieldEmbedder", "kind": "class", "doc": "<p>Base class for all neural network modules.</p>\n\n<p>Your models should also subclass this class.</p>\n\n<p>Modules can also contain other Modules, allowing to nest them in\na tree structure. You can assign the submodules as regular attributes::</p>\n\n<pre><code>import torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\n</code></pre>\n\n<p>Submodules assigned in this way will be registered, and will have their\nparameters converted too when you call <code>to()</code>, etc.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>As per the example above, an <code>__init__()</code> call to the parent class\nmust be made before assignment on the child.</p>\n\n</div>\n\n<p>:ivar training: Boolean represents whether this module is in training or\n                evaluation mode.\n:vartype training: bool</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "source.core.architecture.DiscreteFieldEmbedder.__init__", "modulename": "source.core.architecture", "qualname": "DiscreteFieldEmbedder.__init__", "kind": "function", "doc": "<p>Initialize discrete field embedder.</p>\n\n<p>Args:\n    params (Hyperparameters): The hyperparameters for the architecture.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">params</span><span class=\"p\">:</span> <span class=\"n\">source</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">schema</span><span class=\"o\">.</span><span class=\"n\">Hyperparameters</span>,</span><span class=\"param\">\t<span class=\"n\">field</span><span class=\"p\">:</span> <span class=\"n\">source</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">schema</span><span class=\"o\">.</span><span class=\"n\">Field</span></span>)</span>"}, {"fullname": "source.core.architecture.DiscreteFieldEmbedder.field", "modulename": "source.core.architecture", "qualname": "DiscreteFieldEmbedder.field", "kind": "variable", "doc": "<p></p>\n", "annotation": ": source.core.schema.Field"}, {"fullname": "source.core.architecture.DiscreteFieldEmbedder.embeddings", "modulename": "source.core.architecture", "qualname": "DiscreteFieldEmbedder.embeddings", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "source.core.architecture.DiscreteFieldEmbedder.forward", "modulename": "source.core.architecture", "qualname": "DiscreteFieldEmbedder.forward", "kind": "function", "doc": "<p>Define the computation performed at every call.</p>\n\n<p>Should be overridden by all subclasses.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>Although the recipe for forward pass needs to be defined within\nthis function, one should call the <code>Module</code> instance afterwards\ninstead of this since the former takes care of running the\nregistered hooks while the latter silently ignores them.</p>\n\n</div>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">inputs</span><span class=\"p\">:</span> <span class=\"n\">tensordict</span><span class=\"o\">.</span><span class=\"n\">_td</span><span class=\"o\">.</span><span class=\"n\">TensorDict</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "source.core.architecture.EntityFieldEmbedder", "modulename": "source.core.architecture", "qualname": "EntityFieldEmbedder", "kind": "class", "doc": "<p>Base class for all neural network modules.</p>\n\n<p>Your models should also subclass this class.</p>\n\n<p>Modules can also contain other Modules, allowing to nest them in\na tree structure. You can assign the submodules as regular attributes::</p>\n\n<pre><code>import torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\n</code></pre>\n\n<p>Submodules assigned in this way will be registered, and will have their\nparameters converted too when you call <code>to()</code>, etc.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>As per the example above, an <code>__init__()</code> call to the parent class\nmust be made before assignment on the child.</p>\n\n</div>\n\n<p>:ivar training: Boolean represents whether this module is in training or\n                evaluation mode.\n:vartype training: bool</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "source.core.architecture.EntityFieldEmbedder.__init__", "modulename": "source.core.architecture", "qualname": "EntityFieldEmbedder.__init__", "kind": "function", "doc": "<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">params</span><span class=\"p\">:</span> <span class=\"n\">source</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">schema</span><span class=\"o\">.</span><span class=\"n\">Hyperparameters</span>,</span><span class=\"param\">\t<span class=\"n\">field</span><span class=\"p\">:</span> <span class=\"n\">source</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">schema</span><span class=\"o\">.</span><span class=\"n\">Field</span></span>)</span>"}, {"fullname": "source.core.architecture.EntityFieldEmbedder.field", "modulename": "source.core.architecture", "qualname": "EntityFieldEmbedder.field", "kind": "variable", "doc": "<p></p>\n", "annotation": ": source.core.schema.Field"}, {"fullname": "source.core.architecture.EntityFieldEmbedder.embeddings", "modulename": "source.core.architecture", "qualname": "EntityFieldEmbedder.embeddings", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "source.core.architecture.EntityFieldEmbedder.forward", "modulename": "source.core.architecture", "qualname": "EntityFieldEmbedder.forward", "kind": "function", "doc": "<p>Define the computation performed at every call.</p>\n\n<p>Should be overridden by all subclasses.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>Although the recipe for forward pass needs to be defined within\nthis function, one should call the <code>Module</code> instance afterwards\ninstead of this since the former takes care of running the\nregistered hooks while the latter silently ignores them.</p>\n\n</div>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">inputs</span><span class=\"p\">:</span> <span class=\"n\">tensordict</span><span class=\"o\">.</span><span class=\"n\">_td</span><span class=\"o\">.</span><span class=\"n\">TensorDict</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "source.core.architecture.ContinuousFieldEmbedder", "modulename": "source.core.architecture", "qualname": "ContinuousFieldEmbedder", "kind": "class", "doc": "<p>ContinuousFieldEmbedder is a PyTorch module that encodes features using Fourier features.</p>\n\n<p>Attributes:\n    linear (torch.nn.Linear): A linear layer for transforming the input.\n    positional (torch.nn.Embedding): An embedding layer for positional encoding.\n    weights (Tensor): The weights for the Fourier features.</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "source.core.architecture.ContinuousFieldEmbedder.__init__", "modulename": "source.core.architecture", "qualname": "ContinuousFieldEmbedder.__init__", "kind": "function", "doc": "<p>Initialize continuous field embedder.</p>\n\n<p>Args:\n    params (Hyperparameters): The hyperparameters for the architecture.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">params</span><span class=\"p\">:</span> <span class=\"n\">source</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">schema</span><span class=\"o\">.</span><span class=\"n\">Hyperparameters</span>,</span><span class=\"param\">\t<span class=\"n\">field</span><span class=\"p\">:</span> <span class=\"n\">source</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">schema</span><span class=\"o\">.</span><span class=\"n\">Field</span></span>)</span>"}, {"fullname": "source.core.architecture.ContinuousFieldEmbedder.field", "modulename": "source.core.architecture", "qualname": "ContinuousFieldEmbedder.field", "kind": "variable", "doc": "<p></p>\n", "annotation": ": source.core.schema.Field"}, {"fullname": "source.core.architecture.ContinuousFieldEmbedder.linear", "modulename": "source.core.architecture", "qualname": "ContinuousFieldEmbedder.linear", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "source.core.architecture.ContinuousFieldEmbedder.positional", "modulename": "source.core.architecture", "qualname": "ContinuousFieldEmbedder.positional", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "source.core.architecture.ContinuousFieldEmbedder.forward", "modulename": "source.core.architecture", "qualname": "ContinuousFieldEmbedder.forward", "kind": "function", "doc": "<p>Performs the forward pass of the FourierFeatureEncoder.</p>\n\n<p>Args:\n    inputs (Float[Tensor, \"N L\"]): The input tensor.</p>\n\n<p>Returns:\n    Float[Tensor, \"N L F\"]: The Fourier features of the input.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">inputs</span><span class=\"p\">:</span> <span class=\"n\">tensordict</span><span class=\"o\">.</span><span class=\"n\">_td</span><span class=\"o\">.</span><span class=\"n\">TensorDict</span><span class=\"p\">[</span><span class=\"n\">jaxtyping</span><span class=\"o\">.</span><span class=\"n\">Float</span><span class=\"p\">[</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"s1\">&#39;N L&#39;</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"n\">jaxtyping</span><span class=\"o\">.</span><span class=\"n\">Int</span><span class=\"p\">[</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"s1\">&#39;N L&#39;</span><span class=\"p\">]]</span></span><span class=\"return-annotation\">) -> <span class=\"n\">jaxtyping</span><span class=\"o\">.</span><span class=\"n\">Float</span><span class=\"p\">[</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"s1\">&#39;N L F&#39;</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "source.core.architecture.ModularAttributeEmbeddingSystem", "modulename": "source.core.architecture", "qualname": "ModularAttributeEmbeddingSystem", "kind": "class", "doc": "<p>ModularAttributeEmbeddingSystem is a PyTorch module for embedding fields.</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "source.core.architecture.ModularAttributeEmbeddingSystem.__init__", "modulename": "source.core.architecture", "qualname": "ModularAttributeEmbeddingSystem.__init__", "kind": "function", "doc": "<p>Initialize moduler field embedder.</p>\n\n<p>Args:\n    params (Hyperparameters): The hyperparameters for the architecture.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">params</span><span class=\"p\">:</span> <span class=\"n\">source</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">schema</span><span class=\"o\">.</span><span class=\"n\">Hyperparameters</span></span>)</span>"}, {"fullname": "source.core.architecture.ModularAttributeEmbeddingSystem.embedders", "modulename": "source.core.architecture", "qualname": "ModularAttributeEmbeddingSystem.embedders", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "source.core.architecture.ModularAttributeEmbeddingSystem.forward", "modulename": "source.core.architecture", "qualname": "ModularAttributeEmbeddingSystem.forward", "kind": "function", "doc": "<p>Performs the forward pass of the ModularAttributeEmbeddingSystem.</p>\n\n<p>Args:\n    inputs (TensorDict[Float[Tensor, \"N L\"] | Int[Tensor, \"N L\"]]): The input tensor.</p>\n\n<p>Returns:\n    Float[Tensor, \"N L F\"]: The embedded fields.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">inputs</span><span class=\"p\">:</span> <span class=\"n\">tensordict</span><span class=\"o\">.</span><span class=\"n\">_td</span><span class=\"o\">.</span><span class=\"n\">TensorDict</span></span><span class=\"return-annotation\">) -> <span class=\"n\">jaxtyping</span><span class=\"o\">.</span><span class=\"n\">Float</span><span class=\"p\">[</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"s1\">&#39;N L F C&#39;</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "source.core.architecture.FieldEncoder", "modulename": "source.core.architecture", "qualname": "FieldEncoder", "kind": "class", "doc": "<p>FieldEncoder is a PyTorch module for encoding fields.</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "source.core.architecture.FieldEncoder.__init__", "modulename": "source.core.architecture", "qualname": "FieldEncoder.__init__", "kind": "function", "doc": "<p>Initialize field transformer encoder.</p>\n\n<p>Args:\n    params (Hyperparameters): The hyperparameters for the architecture.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">params</span><span class=\"p\">:</span> <span class=\"n\">source</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">schema</span><span class=\"o\">.</span><span class=\"n\">Hyperparameters</span></span>)</span>"}, {"fullname": "source.core.architecture.FieldEncoder.H", "modulename": "source.core.architecture", "qualname": "FieldEncoder.H", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "source.core.architecture.FieldEncoder.encoder", "modulename": "source.core.architecture", "qualname": "FieldEncoder.encoder", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "source.core.architecture.FieldEncoder.positional", "modulename": "source.core.architecture", "qualname": "FieldEncoder.positional", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "source.core.architecture.FieldEncoder.forward", "modulename": "source.core.architecture", "qualname": "FieldEncoder.forward", "kind": "function", "doc": "<p>Performs the forward pass of the FieldEncoder.</p>\n\n<p>Args:\n    inputs (Float[Tensor, \"N L F C\"]): The input tensor.</p>\n\n<p>Returns:\n    Float[Tensor, \"N L F C\"]: The encoded fields.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">fields</span><span class=\"p\">:</span> <span class=\"n\">jaxtyping</span><span class=\"o\">.</span><span class=\"n\">Float</span><span class=\"p\">[</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"s1\">&#39;N L F C&#39;</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">mask</span><span class=\"p\">:</span> <span class=\"n\">jaxtyping</span><span class=\"o\">.</span><span class=\"n\">Bool</span><span class=\"p\">[</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"s1\">&#39;NL F F&#39;</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"n\">jaxtyping</span><span class=\"o\">.</span><span class=\"n\">Float</span><span class=\"p\">[</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"s1\">&#39;N L FC&#39;</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "source.core.architecture.EventEncoder", "modulename": "source.core.architecture", "qualname": "EventEncoder", "kind": "class", "doc": "<p>EventEncoder is a PyTorch module for encoding events.</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "source.core.architecture.EventEncoder.__init__", "modulename": "source.core.architecture", "qualname": "EventEncoder.__init__", "kind": "function", "doc": "<p>Initialize the event transformer encoder.</p>\n\n<p>Args:\n    params (Hyperparameters): The hyperparameters for the architecture.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">params</span><span class=\"p\">:</span> <span class=\"n\">source</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">schema</span><span class=\"o\">.</span><span class=\"n\">Hyperparameters</span></span>)</span>"}, {"fullname": "source.core.architecture.EventEncoder.H", "modulename": "source.core.architecture", "qualname": "EventEncoder.H", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "source.core.architecture.EventEncoder.encoder", "modulename": "source.core.architecture", "qualname": "EventEncoder.encoder", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "source.core.architecture.EventEncoder.positional", "modulename": "source.core.architecture", "qualname": "EventEncoder.positional", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "source.core.architecture.EventEncoder.class_token", "modulename": "source.core.architecture", "qualname": "EventEncoder.class_token", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "source.core.architecture.EventEncoder.forward", "modulename": "source.core.architecture", "qualname": "EventEncoder.forward", "kind": "function", "doc": "<p>Define the computation performed at every call.</p>\n\n<p>Should be overridden by all subclasses.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>Although the recipe for forward pass needs to be defined within\nthis function, one should call the <code>Module</code> instance afterwards\ninstead of this since the former takes care of running the\nregistered hooks while the latter silently ignores them.</p>\n\n</div>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">events</span><span class=\"p\">:</span> <span class=\"n\">jaxtyping</span><span class=\"o\">.</span><span class=\"n\">Float</span><span class=\"p\">[</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"s1\">&#39;N L FC&#39;</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">mask</span><span class=\"p\">:</span> <span class=\"n\">jaxtyping</span><span class=\"o\">.</span><span class=\"n\">Bool</span><span class=\"p\">[</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"s1\">&#39;N L L&#39;</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">jaxtyping</span><span class=\"o\">.</span><span class=\"n\">Float</span><span class=\"p\">[</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"s1\">&#39;N FC&#39;</span><span class=\"p\">],</span> <span class=\"n\">jaxtyping</span><span class=\"o\">.</span><span class=\"n\">Float</span><span class=\"p\">[</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"s1\">&#39;N L FC&#39;</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "source.core.architecture.EventDecoder", "modulename": "source.core.architecture", "qualname": "EventDecoder", "kind": "class", "doc": "<p>EventDecoder is a PyTorch module for decoding masked events from their contextualized representations.</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "source.core.architecture.EventDecoder.__init__", "modulename": "source.core.architecture", "qualname": "EventDecoder.__init__", "kind": "function", "doc": "<p>Initialize the event decoder.</p>\n\n<p>Args:\n    params (Hyperparameters): The hyperparameters for the architecture.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">params</span><span class=\"p\">:</span> <span class=\"n\">source</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">schema</span><span class=\"o\">.</span><span class=\"n\">Hyperparameters</span></span>)</span>"}, {"fullname": "source.core.architecture.EventDecoder.projections", "modulename": "source.core.architecture", "qualname": "EventDecoder.projections", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "source.core.architecture.EventDecoder.forward", "modulename": "source.core.architecture", "qualname": "EventDecoder.forward", "kind": "function", "doc": "<p>Performs the forward pass of the EventDecoder.</p>\n\n<p>Args:\n    inputs (Float[Tensor, \"N L FC\"]): The input tensor.</p>\n\n<p>Returns:\n    TensorDict[Float[Tensor, \"N L ?\"]]: A dictionary of output tensors for each field.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">inputs</span><span class=\"p\">:</span> <span class=\"n\">jaxtyping</span><span class=\"o\">.</span><span class=\"n\">Float</span><span class=\"p\">[</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"s1\">&#39;N L FC&#39;</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"n\">tensordict</span><span class=\"o\">.</span><span class=\"n\">_td</span><span class=\"o\">.</span><span class=\"n\">TensorDict</span><span class=\"p\">[</span><span class=\"n\">jaxtyping</span><span class=\"o\">.</span><span class=\"n\">Float</span><span class=\"p\">[</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"s1\">&#39;N L ?&#39;</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "source.core.architecture.DecisionHead", "modulename": "source.core.architecture", "qualname": "DecisionHead", "kind": "class", "doc": "<p>A PyTorch module representing a classification head. This module is used to map the encoded fields to the target classes.</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "source.core.architecture.DecisionHead.__init__", "modulename": "source.core.architecture", "qualname": "DecisionHead.__init__", "kind": "function", "doc": "<p>Initialize classification decision head.</p>\n\n<p>Args:\n    params (Hyperparameters): The hyperparameters for the architecture.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">params</span><span class=\"p\">:</span> <span class=\"n\">source</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">schema</span><span class=\"o\">.</span><span class=\"n\">Hyperparameters</span></span>)</span>"}, {"fullname": "source.core.architecture.DecisionHead.mlp", "modulename": "source.core.architecture", "qualname": "DecisionHead.mlp", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "source.core.architecture.DecisionHead.forward", "modulename": "source.core.architecture", "qualname": "DecisionHead.forward", "kind": "function", "doc": "<p>Defines the forward pass of the ClassificationHead.</p>\n\n<p>Args:\n    inputs (torch.Tensor): The input tensor.</p>\n\n<p>Returns:\n    torch.Tensor: The output of the MLP.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">inputs</span><span class=\"p\">:</span> <span class=\"n\">jaxtyping</span><span class=\"o\">.</span><span class=\"n\">Float</span><span class=\"p\">[</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"s1\">&#39;N FC&#39;</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"n\">jaxtyping</span><span class=\"o\">.</span><span class=\"n\">Float</span><span class=\"p\">[</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"s1\">&#39;N T&#39;</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "source.core.architecture.create_metrics", "modulename": "source.core.architecture", "qualname": "create_metrics", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">params</span><span class=\"p\">:</span> <span class=\"n\">source</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">schema</span><span class=\"o\">.</span><span class=\"n\">Hyperparameters</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">container</span><span class=\"o\">.</span><span class=\"n\">ModuleDict</span>:</span></span>", "funcdef": "def"}, {"fullname": "source.core.architecture.step", "modulename": "source.core.architecture", "qualname": "step", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span><span class=\"p\">:</span> <span class=\"n\">source</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">architecture</span><span class=\"o\">.</span><span class=\"n\">SequenceModule</span>,</span><span class=\"param\">\t<span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">tensordict</span><span class=\"o\">.</span><span class=\"n\">_td</span><span class=\"o\">.</span><span class=\"n\">TensorDict</span> <span class=\"o\">|</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">tensordict</span><span class=\"o\">.</span><span class=\"n\">_td</span><span class=\"o\">.</span><span class=\"n\">TensorDict</span><span class=\"p\">,</span> <span class=\"n\">tensordict</span><span class=\"o\">.</span><span class=\"n\">_td</span><span class=\"o\">.</span><span class=\"n\">TensorDict</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">strata</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span> <span class=\"o\">|</span> <span class=\"n\">tensordict</span><span class=\"o\">.</span><span class=\"n\">_td</span><span class=\"o\">.</span><span class=\"n\">TensorDict</span>:</span></span>", "funcdef": "def"}, {"fullname": "source.core.architecture.dataloader", "modulename": "source.core.architecture", "qualname": "dataloader", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span><span class=\"p\">:</span> <span class=\"n\">source</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">architecture</span><span class=\"o\">.</span><span class=\"n\">SequenceModule</span>,</span><span class=\"param\">\t<span class=\"n\">strata</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torchdata</span><span class=\"o\">.</span><span class=\"n\">dataloader2</span><span class=\"o\">.</span><span class=\"n\">dataloader2</span><span class=\"o\">.</span><span class=\"n\">DataLoader2</span>:</span></span>", "funcdef": "def"}, {"fullname": "source.core.architecture.SequenceModule", "modulename": "source.core.architecture", "qualname": "SequenceModule", "kind": "class", "doc": "<p>Hooks to be used in LightningModule.</p>\n", "bases": "lightning.pytorch.core.module.LightningModule"}, {"fullname": "source.core.architecture.SequenceModule.__init__", "modulename": "source.core.architecture", "qualname": "SequenceModule.__init__", "kind": "function", "doc": "<p>Attributes:\n    prepare_data_per_node:\n        If True, each LOCAL_RANK=0 will call prepare data.\n        Otherwise only NODE_RANK=0, LOCAL_RANK=0 will prepare data.\n    allow_zero_length_dataloader_with_multiple_devices:\n        If True, dataloader with zero length within local rank is allowed.\n        Default value is False.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">datapath</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span>,</span><span class=\"param\">\t<span class=\"n\">params</span><span class=\"p\">:</span> <span class=\"n\">source</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">schema</span><span class=\"o\">.</span><span class=\"n\">Hyperparameters</span>,</span><span class=\"param\">\t<span class=\"n\">digests</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">tdigest</span><span class=\"o\">.</span><span class=\"n\">tdigest</span><span class=\"o\">.</span><span class=\"n\">TDigest</span><span class=\"p\">]</span></span>)</span>"}, {"fullname": "source.core.architecture.SequenceModule.datapath", "modulename": "source.core.architecture", "qualname": "SequenceModule.datapath", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str | os.PathLike"}, {"fullname": "source.core.architecture.SequenceModule.params", "modulename": "source.core.architecture", "qualname": "SequenceModule.params", "kind": "variable", "doc": "<p></p>\n", "annotation": ": source.core.schema.Hyperparameters"}, {"fullname": "source.core.architecture.SequenceModule.modular_field_embedder", "modulename": "source.core.architecture", "qualname": "SequenceModule.modular_field_embedder", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "source.core.architecture.SequenceModule.field_encoder", "modulename": "source.core.architecture", "qualname": "SequenceModule.field_encoder", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "source.core.architecture.SequenceModule.event_encoder", "modulename": "source.core.architecture", "qualname": "SequenceModule.event_encoder", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "source.core.architecture.SequenceModule.event_decoder", "modulename": "source.core.architecture", "qualname": "SequenceModule.event_decoder", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "source.core.architecture.SequenceModule.decision_head", "modulename": "source.core.architecture", "qualname": "SequenceModule.decision_head", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "source.core.architecture.SequenceModule.metrics", "modulename": "source.core.architecture", "qualname": "SequenceModule.metrics", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "source.core.architecture.SequenceModule.digests", "modulename": "source.core.architecture", "qualname": "SequenceModule.digests", "kind": "variable", "doc": "<p></p>\n", "annotation": ": dict[str, tdigest.tdigest.TDigest]"}, {"fullname": "source.core.architecture.SequenceModule.loaders", "modulename": "source.core.architecture", "qualname": "SequenceModule.loaders", "kind": "variable", "doc": "<p></p>\n", "annotation": ": list[torchdata.dataloader2.dataloader2.DataLoader2]"}, {"fullname": "source.core.architecture.SequenceModule.pipes", "modulename": "source.core.architecture", "qualname": "SequenceModule.pipes", "kind": "variable", "doc": "<p></p>\n", "annotation": ": dict[str, torch.utils.data.datapipes.datapipe.IterDataPipe]"}, {"fullname": "source.core.architecture.SequenceModule.example_input_array", "modulename": "source.core.architecture", "qualname": "SequenceModule.example_input_array", "kind": "variable", "doc": "<p>The example input array is a specification of what the module can consume in the <code>forward()</code> method. The\nreturn type is interpreted as follows:</p>\n\n<ul>\n<li>Single tensor: It is assumed the model takes a single argument, i.e.,\n<code>model.forward(model.example_input_array)</code></li>\n<li>Tuple: The input array should be interpreted as a sequence of positional arguments, i.e.,\n<code>model.forward(*model.example_input_array)</code></li>\n<li>Dict: The input array represents named keyword arguments, i.e.,\n<code>model.forward(**model.example_input_array)</code></li>\n</ul>\n", "annotation": ": Union[torch.Tensor, Tuple, Dict, NoneType]"}, {"fullname": "source.core.architecture.SequenceModule.forward", "modulename": "source.core.architecture", "qualname": "SequenceModule.forward", "kind": "function", "doc": "<p>Performs the forward pass of the encoder.</p>\n\n<p>Args:\n    events (Float[Tensor, \"N L FC\"]): The input tensor.</p>\n\n<p>Returns:\n    tuple[Tensor, TensorDict]:\n    A tuple containing two tensors. The first tensor represents the supervised classification predictions and the second tensor represents the decoded, reconstructed events.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">inputs</span><span class=\"p\">:</span> <span class=\"n\">tensordict</span><span class=\"o\">.</span><span class=\"n\">_td</span><span class=\"o\">.</span><span class=\"n\">TensorDict</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">jaxtyping</span><span class=\"o\">.</span><span class=\"n\">Float</span><span class=\"p\">[</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"s1\">&#39;N T&#39;</span><span class=\"p\">],</span> <span class=\"n\">tensordict</span><span class=\"o\">.</span><span class=\"n\">_td</span><span class=\"o\">.</span><span class=\"n\">TensorDict</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "source.core.architecture.SequenceModule.configure_optimizers", "modulename": "source.core.architecture", "qualname": "SequenceModule.configure_optimizers", "kind": "function", "doc": "<p>Choose what optimizers and learning-rate schedulers to use in your optimization. Normally you'd need one.\nBut in the case of GANs or similar you might have multiple. Optimization with multiple optimizers only works in\nthe manual optimization mode.</p>\n\n<p>Return:\n    Any of these 6 options.</p>\n\n<pre><code>- **Single optimizer**.\n- **List or Tuple** of optimizers.\n- **Two lists** - The first list has multiple optimizers, and the second has multiple LR schedulers\n  (or multiple ``lr_scheduler_config``).\n- **Dictionary**, with an ``\"optimizer\"`` key, and (optionally) a ``\"lr_scheduler\"``\n  key whose value is a single LR scheduler or ``lr_scheduler_config``.\n- **None** - Fit will run without any optimizer.\n</code></pre>\n\n<p>The <code>lr_scheduler_config</code> is a dictionary which contains the scheduler and its associated configuration.\nThe default configuration is shown below.</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"n\">lr_scheduler_config</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n    <span class=\"c1\"># REQUIRED: The scheduler instance</span>\n    <span class=\"s2\">&quot;scheduler&quot;</span><span class=\"p\">:</span> <span class=\"n\">lr_scheduler</span><span class=\"p\">,</span>\n    <span class=\"c1\"># The unit of the scheduler&#39;s step size, could also be &#39;step&#39;.</span>\n    <span class=\"c1\"># &#39;epoch&#39; updates the scheduler on epoch end whereas &#39;step&#39;</span>\n    <span class=\"c1\"># updates it after a optimizer update.</span>\n    <span class=\"s2\">&quot;interval&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;epoch&quot;</span><span class=\"p\">,</span>\n    <span class=\"c1\"># How many epochs/steps should pass between calls to</span>\n    <span class=\"c1\"># `scheduler.step()`. 1 corresponds to updating the learning</span>\n    <span class=\"c1\"># rate after every epoch/step.</span>\n    <span class=\"s2\">&quot;frequency&quot;</span><span class=\"p\">:</span> <span class=\"mi\">1</span><span class=\"p\">,</span>\n    <span class=\"c1\"># Metric to to monitor for schedulers like `ReduceLROnPlateau`</span>\n    <span class=\"s2\">&quot;monitor&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;val_loss&quot;</span><span class=\"p\">,</span>\n    <span class=\"c1\"># If set to `True`, will enforce that the value specified &#39;monitor&#39;</span>\n    <span class=\"c1\"># is available when the scheduler is updated, thus stopping</span>\n    <span class=\"c1\"># training if not found. If set to `False`, it will only produce a warning</span>\n    <span class=\"s2\">&quot;strict&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span>\n    <span class=\"c1\"># If using the `LearningRateMonitor` callback to monitor the</span>\n    <span class=\"c1\"># learning rate progress, this keyword can be used to specify</span>\n    <span class=\"c1\"># a custom logged name</span>\n    <span class=\"s2\">&quot;name&quot;</span><span class=\"p\">:</span> <span class=\"kc\">None</span><span class=\"p\">,</span>\n<span class=\"p\">}</span>\n</code></pre>\n</div>\n\n<p>When there are schedulers in which the <code>.step()</code> method is conditioned on a value, such as the\n<code>torch.optim.lr_scheduler.ReduceLROnPlateau</code> scheduler, Lightning requires that the\n<code>lr_scheduler_config</code> contains the keyword <code>\"monitor\"</code> set to the metric name that the scheduler\nshould be conditioned on.</p>\n\n<p>.. testcode::</p>\n\n<pre><code># The ReduceLROnPlateau scheduler requires a monitor\ndef configure_optimizers(self):\n    optimizer = Adam(...)\n    return {\n        \"optimizer\": optimizer,\n        \"lr_scheduler\": {\n            \"scheduler\": ReduceLROnPlateau(optimizer, ...),\n            \"monitor\": \"metric_to_track\",\n            \"frequency\": \"indicates how often the metric is updated\",\n            # If \"monitor\" references validation metrics, then \"frequency\" should be set to a\n            # multiple of \"trainer.check_val_every_n_epoch\".\n        },\n    }\n\n\n# In the case of two optimizers, only one using the ReduceLROnPlateau scheduler\ndef configure_optimizers(self):\n    optimizer1 = Adam(...)\n    optimizer2 = SGD(...)\n    scheduler1 = ReduceLROnPlateau(optimizer1, ...)\n    scheduler2 = LambdaLR(optimizer2, ...)\n    return (\n        {\n            \"optimizer\": optimizer1,\n            \"lr_scheduler\": {\n                \"scheduler\": scheduler1,\n                \"monitor\": \"metric_to_track\",\n            },\n        },\n        {\"optimizer\": optimizer2, \"lr_scheduler\": scheduler2},\n    )\n</code></pre>\n\n<p>Metrics can be made available to monitor by simply logging it using\n<code>self.log('metric_to_track', metric_val)</code> in your <code>~lightning.pytorch.core.LightningModule</code>.</p>\n\n<p>Note:\n    Some things to know:</p>\n\n<pre><code>- Lightning calls ``.backward()`` and ``.step()`` automatically in case of automatic optimization.\n- If a learning rate scheduler is specified in ``configure_optimizers()`` with key\n  ``\"interval\"`` (default \"epoch\") in the scheduler configuration, Lightning will call\n  the scheduler's ``.step()`` method automatically in case of automatic optimization.\n- If you use 16-bit precision (``precision=16``), Lightning will automatically handle the optimizer.\n- If you use `torch.optim.LBFGS`, Lightning handles the closure function automatically for you.\n- If you use multiple optimizers, you will have to switch to 'manual optimization' mode and step them\n  yourself.\n- If you need to control how often the optimizer steps, override the `optimizer_step()` hook.\n</code></pre>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">optim</span><span class=\"o\">.</span><span class=\"n\">adamw</span><span class=\"o\">.</span><span class=\"n\">AdamW</span>:</span></span>", "funcdef": "def"}, {"fullname": "source.core.architecture.SequenceModule.complexity", "modulename": "source.core.architecture", "qualname": "SequenceModule.complexity", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"nb\">format</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"nb\">int</span>:</span></span>", "funcdef": "def"}, {"fullname": "source.core.architecture.SequenceModule.training_step", "modulename": "source.core.architecture", "qualname": "SequenceModule.training_step", "kind": "function", "doc": "<p>Method descriptor with partial application of the given arguments\nand keywords.</p>\n\n<p>Supports wrapping existing descriptors and handles non-descriptor\ncallables as instance methods.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">unknown</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "source.core.architecture.SequenceModule.validation_step", "modulename": "source.core.architecture", "qualname": "SequenceModule.validation_step", "kind": "function", "doc": "<p>Method descriptor with partial application of the given arguments\nand keywords.</p>\n\n<p>Supports wrapping existing descriptors and handles non-descriptor\ncallables as instance methods.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">unknown</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "source.core.architecture.SequenceModule.testing_step", "modulename": "source.core.architecture", "qualname": "SequenceModule.testing_step", "kind": "function", "doc": "<p>Method descriptor with partial application of the given arguments\nand keywords.</p>\n\n<p>Supports wrapping existing descriptors and handles non-descriptor\ncallables as instance methods.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">unknown</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "source.core.architecture.SequenceModule.predict_step", "modulename": "source.core.architecture", "qualname": "SequenceModule.predict_step", "kind": "function", "doc": "<p>Method descriptor with partial application of the given arguments\nand keywords.</p>\n\n<p>Supports wrapping existing descriptors and handles non-descriptor\ncallables as instance methods.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">unknown</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "source.core.architecture.SequenceModule.setup", "modulename": "source.core.architecture", "qualname": "SequenceModule.setup", "kind": "function", "doc": "<p>Called at the beginning of fit (train + validate), validate, test, or predict. This is a good hook when you\nneed to build models dynamically or adjust something about them. This hook is called on every process when\nusing DDP.</p>\n\n<p>Args:\n    stage: either <code>'fit'</code>, <code>'validate'</code>, <code>'test'</code>, or <code>'predict'</code></p>\n\n<p>Example::</p>\n\n<pre><code>class LitModel(...):\n    def __init__(self):\n        self.l1 = None\n\n    def prepare_data(self):\n        download_data()\n        tokenize()\n\n        # don't do this\n        self.something = else\n\n    def setup(self, stage):\n        data = load_data(...)\n        self.l1 = nn.Linear(28, data.num_classes)\n</code></pre>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">stage</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "source.core.architecture.SequenceModule.teardown", "modulename": "source.core.architecture", "qualname": "SequenceModule.teardown", "kind": "function", "doc": "<p>Called at the end of fit (train + validate), validate, test, or predict.</p>\n\n<p>Args:\n    stage: either <code>'fit'</code>, <code>'validate'</code>, <code>'test'</code>, or <code>'predict'</code></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">stage</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "source.core.architecture.SequenceModule.train_dataloader", "modulename": "source.core.architecture", "qualname": "SequenceModule.train_dataloader", "kind": "function", "doc": "<p>Method descriptor with partial application of the given arguments\nand keywords.</p>\n\n<p>Supports wrapping existing descriptors and handles non-descriptor\ncallables as instance methods.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">unknown</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "source.core.architecture.SequenceModule.val_dataloader", "modulename": "source.core.architecture", "qualname": "SequenceModule.val_dataloader", "kind": "function", "doc": "<p>Method descriptor with partial application of the given arguments\nand keywords.</p>\n\n<p>Supports wrapping existing descriptors and handles non-descriptor\ncallables as instance methods.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">unknown</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "source.core.architecture.SequenceModule.test_dataloader", "modulename": "source.core.architecture", "qualname": "SequenceModule.test_dataloader", "kind": "function", "doc": "<p>Method descriptor with partial application of the given arguments\nand keywords.</p>\n\n<p>Supports wrapping existing descriptors and handles non-descriptor\ncallables as instance methods.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">unknown</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "source.core.architecture.SequenceModule.predict_dataloader", "modulename": "source.core.architecture", "qualname": "SequenceModule.predict_dataloader", "kind": "function", "doc": "<p>Method descriptor with partial application of the given arguments\nand keywords.</p>\n\n<p>Supports wrapping existing descriptors and handles non-descriptor\ncallables as instance methods.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">unknown</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "source.core.architecture.SequenceModule.mode", "modulename": "source.core.architecture", "qualname": "SequenceModule.mode", "kind": "variable", "doc": "<p></p>\n"}];

    // mirrored in build-search-index.js (part 1)
    // Also split on html tags. this is a cheap heuristic, but good enough.
    elasticlunr.tokenizer.setSeperator(/[\s\-.;&_'"=,()]+|<[^>]*>/);

    let searchIndex;
    if (docs._isPrebuiltIndex) {
        console.info("using precompiled search index");
        searchIndex = elasticlunr.Index.load(docs);
    } else {
        console.time("building search index");
        // mirrored in build-search-index.js (part 2)
        searchIndex = elasticlunr(function () {
            this.pipeline.remove(elasticlunr.stemmer);
            this.pipeline.remove(elasticlunr.stopWordFilter);
            this.addField("qualname");
            this.addField("fullname");
            this.addField("annotation");
            this.addField("default_value");
            this.addField("signature");
            this.addField("bases");
            this.addField("doc");
            this.setRef("fullname");
        });
        for (let doc of docs) {
            searchIndex.addDoc(doc);
        }
        console.timeEnd("building search index");
    }

    return (term) => searchIndex.search(term, {
        fields: {
            qualname: {boost: 4},
            fullname: {boost: 2},
            annotation: {boost: 2},
            default_value: {boost: 2},
            signature: {boost: 2},
            bases: {boost: 2},
            doc: {boost: 1},
        },
        expand: true
    });
})();