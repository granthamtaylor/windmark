n_pretrain_steps: 192
n_finetune_steps: 192
n_context: 64
batch_size: 128
d_field: 64
max_pretrain_epochs: 960
max_finetune_epochs: 960
n_layers_field_encoder: 1
n_heads_field_encoder: 4
n_layers_event_encoder: 1
n_heads_event_encoder: 4
n_epochs_frozen: 2
learning_rate: 0.000005
quantile_smoothing: 0.0001
n_quantiles: 64
p_mask_event: 0.00
p_mask_field: 0.00
p_mask_static: 0.00
dropout: 0.0
patience: 32
